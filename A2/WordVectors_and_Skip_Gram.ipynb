{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "pnu0uUOXUv8p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnu0uUOXUv8p",
        "outputId": "4e059761-f4ad-4daa-ae89-24f01fa7a90a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.17.0\n",
            "  Downloading https://download.pytorch.org/whl/torchtext-0.17.0%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.66.5)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.17.0)\n",
            "  Downloading https://download.pytorch.org/whl/torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchdata, torchaudio, torchtext\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu121\n",
            "    Uninstalling torch-2.4.1+cu121:\n",
            "      Successfully uninstalled torch-2.4.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.1+cu121\n",
            "    Uninstalling torchvision-0.19.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.1+cu121\n",
            "    Uninstalling torchaudio-2.4.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.4.1+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 torch-2.2.0+cu121 torchaudio-2.2.0+cu121 torchdata-0.7.1 torchtext-0.17.0+cpu torchvision-0.17.0+cu121 triton-2.2.0\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# You need to downgrade the colab torch version to make this work\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 torchtext==0.17.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "_6U-IsqFVIJe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6U-IsqFVIJe",
        "outputId": "a5aebd53-bac5-4e67-a69f-1b1a90d12f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 21 11:40:07 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "080d50fc",
      "metadata": {
        "id": "080d50fc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import DATASETS\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader\n",
        "from torch import FloatTensor as FT\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "\n",
        "# Get the interactive Tools for Matplotlib\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "p-8th25fUu25",
      "metadata": {
        "id": "p-8th25fUu25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "29a9d66b",
      "metadata": {
        "id": "29a9d66b"
      },
      "source": [
        "# Visualize some pretrained models.\n",
        "- See: https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b2fe2dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2fe2dcd",
        "outputId": "49fc74af-8018-46e1-ded8-d439b612454b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ],
      "source": [
        "# Pretrained models!\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ca7a34b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ca7a34b0",
        "outputId": "ef0dffbb-3db6-4fbf-b26b-f025f8227f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d3a43458ae88>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/gensim-data/word2vec-google-news-300/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m             _word2vec_read_binary(\n\u001b[0m\u001b[1;32m   2066\u001b[0m                 \u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_chunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         \u001b[0mnew_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         processed_words, chunk = _add_bytes_to_kv(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[1;32m   1962\u001b[0m         \u001b[0mtot_processed_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocessed_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_add_bytes_to_kv\u001b[0;34m(kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1943\u001b[0;31m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbytes_per_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m         \u001b[0mprocessed_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"duplicate word '%s' in word2vec file, ignoring all but first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m     \u001b[0mword_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a843725",
      "metadata": {
        "id": "9a843725"
      },
      "outputs": [],
      "source": [
        "# This takes the vector for a word and gets the most similar words to this vector.\n",
        "model.most_similar('banana')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c0e202b",
      "metadata": {
        "id": "5c0e202b"
      },
      "outputs": [],
      "source": [
        "model.most_similar('obama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c94d3c",
      "metadata": {
        "id": "25c94d3c"
      },
      "outputs": [],
      "source": [
        "# x2 - x1 = y2 - y1 ... So y2 = x2 - x1 + y1 ... Consider all neighbors of x2 - x1 + y1, get y2.\n",
        "def analogy(model, x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a020de3",
      "metadata": {
        "id": "1a020de3"
      },
      "outputs": [],
      "source": [
        "analogy(model, 'japan', 'japanese', 'australia')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f23e72a",
      "metadata": {
        "id": "6f23e72a"
      },
      "outputs": [],
      "source": [
        "analogy(model, 'obama', 'clinton', 'reagan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b520f6",
      "metadata": {
        "id": "09b520f6"
      },
      "outputs": [],
      "source": [
        "analogy(model, 'tall', 'tallest', 'long')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0676a8",
      "metadata": {
        "id": "fb0676a8"
      },
      "outputs": [],
      "source": [
        "# Which word from the given list doesn’t go with the others?\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078884bd",
      "metadata": {
        "id": "078884bd"
      },
      "outputs": [],
      "source": [
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
        "        else:\n",
        "            words = [ word for word in model.vocab ]\n",
        "\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ddabe0a",
      "metadata": {
        "id": "3ddabe0a"
      },
      "source": [
        "### Let's project to two dimensions and see if thee are any patterns.\n",
        "We see some clusters:\n",
        "- Countries are together.\n",
        "- Drinks are together.\n",
        "- Foods.\n",
        "- Etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc836fea",
      "metadata": {
        "id": "cc836fea"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(\n",
        "    model,\n",
        "    [\n",
        "        'coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
        "        'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
        "        'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
        "        'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
        "        'france', 'germany', 'hungary', 'france', 'australia', 'fiji', 'china',\n",
        "        'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
        "        'school', 'college', 'university', 'institute'\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5419c32",
      "metadata": {
        "id": "f5419c32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "66eb271d",
      "metadata": {
        "id": "66eb271d"
      },
      "source": [
        "### Information\n",
        "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
        "- torchtext documentation: https://pytorch.org/text/stable/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31b412ca",
      "metadata": {
        "id": "31b412ca"
      },
      "outputs": [],
      "source": [
        "# Where do I want to run my job. You can do \"cuda\" on linux machines.\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else  \"cpu\"\n",
        "# The batch size in Adam or SGD.\n",
        "BATCH_SIZE = 512\n",
        "# Number of epochs.\n",
        "NUM_EPOCHS = 10\n",
        "# Predict from 2 words the inner word for CBOW.\n",
        "# I.e. I'll have a window like [\"a\", \"b\", \"c\"] of continuous text (each is a word).\n",
        "# We'll predict each of wc = [\"a\", \"c\"] from \"b\" = wc for Skip-Gram.\n",
        "# For CBOW, we'll use [\"a\", \"c\"] to predict \"b\" = wo.\n",
        "\n",
        "# This is *NOT* used. This is \"m\" in lecture. It is random for each center word. See below.\n",
        "WINDOW = 1\n",
        "\n",
        "# Negative samples.\n",
        "K = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f1e88237",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1e88237",
        "outputId": "6bd96878-9869-4332-b379-edf3829f1c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3856763a",
      "metadata": {
        "id": "3856763a"
      },
      "source": [
        "The text8 Wikipedia corpus. 100M characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ndMLypaI40qn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndMLypaI40qn",
        "outputId": "d580ebfa-0a53-4344-f26a-206e900ed944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "87fa8c9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87fa8c9b",
        "outputId": "27419e7c-b0e4-4586-f6a1-654334334ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "du: cannot access 'text8': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!du -h text8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "33db9751",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33db9751",
        "outputId": "1de03a9a-c033-44e6-cbd6-ed686b0c8357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000000\n"
          ]
        }
      ],
      "source": [
        "f = open('/content/drive/MyDrive/text8/text8', 'r')\n",
        "text = f.read()\n",
        "# One big string of size 100M.\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1ac4e787",
      "metadata": {
        "id": "1ac4e787"
      },
      "outputs": [],
      "source": [
        "punc = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_\\'{|}~\\t\\n'\n",
        "\n",
        "# Can do regular expressions here too.\n",
        "for c in punc:\n",
        "    if c in text:\n",
        "        text.replace(c, ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a901365f",
      "metadata": {
        "id": "a901365f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f3f2f4d4",
      "metadata": {
        "id": "f3f2f4d4"
      },
      "outputs": [],
      "source": [
        "# A very crude tokenizer you get for free: lower case and also split on spaces.\n",
        "TOKENIZER = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a7695b95",
      "metadata": {
        "id": "a7695b95"
      },
      "outputs": [],
      "source": [
        "words = TOKENIZER(text)\n",
        "f = Counter(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2c5f9ce8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5f9ce8",
        "outputId": "f9ba78ea-28ac-4c30-ead0-c8dc7c485ce9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17005207"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6128f0aa",
      "metadata": {
        "id": "6128f0aa"
      },
      "outputs": [],
      "source": [
        "# Do a very crude filter on the text which removes all words which are rare\n",
        "text = [word for word in words if f[word] > 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d089d13d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d089d13d",
        "outputId": "33322e7d-e2f1-4bc8-9de1-ac509135aded"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anarchism', 'originated', 'as', 'a', 'term']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "text[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "11f47bda",
      "metadata": {
        "id": "11f47bda"
      },
      "outputs": [],
      "source": [
        "VOCAB = build_vocab_from_iterator([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9df4fcbe",
      "metadata": {
        "id": "9df4fcbe"
      },
      "outputs": [],
      "source": [
        "# word -> int hash map.\n",
        "stoi = VOCAB.get_stoi()\n",
        "# int -> word hash map.\n",
        "itos = VOCAB.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "53bba0d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53bba0d1",
        "outputId": "acddf079-4b74-42fc-ca39-21f92a5914a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "stoi['as']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8e2ff447",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e2ff447",
        "outputId": "8aaf5686-52b1-4275-87d5-3d0eb6fd6e3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63641"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Total number of words.\n",
        "len(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b4f4d652",
      "metadata": {
        "id": "b4f4d652"
      },
      "outputs": [],
      "source": [
        "f = Counter(text)\n",
        "# This is the probability that we pick a word in the corpus.\n",
        "z = {word: f[word] / len(text) for word in f}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "13c8c565",
      "metadata": {
        "id": "13c8c565"
      },
      "outputs": [],
      "source": [
        "threshold = 1e-5\n",
        "# Probability that word is kept while subsampling.\n",
        "# This is explained here and sightly differet from the paper: http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
        "p_keep = {word: (np.sqrt(z[word] / 0.001) + 1)*(0.0001 / z[word]) for word in f}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "368bb139",
      "metadata": {
        "id": "368bb139"
      },
      "outputs": [],
      "source": [
        "# This is in the integer space.\n",
        "train_dataset = [word for word in text if random.random() < p_keep[word]]\n",
        "\n",
        "# Rebuild the vocabulary.\n",
        "VOCAB = build_vocab_from_iterator([train_dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bcf29691",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcf29691",
        "outputId": "fa3b251a-d69f-4f64-81ce-1cb665f5ba68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7847594"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "507940fc",
      "metadata": {
        "id": "507940fc"
      },
      "outputs": [],
      "source": [
        "# word -> int mapping.\n",
        "stoi = VOCAB.get_stoi()\n",
        "# int -> word mapping.\n",
        "itos = VOCAB.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3a444e83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a444e83",
        "outputId": "1ef1028e-8711-46ac-d3c9-353112238295"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63641"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# The vocabulary size after we do all the filters.\n",
        "len(VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "41cbbc14",
      "metadata": {
        "id": "41cbbc14"
      },
      "outputs": [],
      "source": [
        "# The probability we draw something for negative sampling.\n",
        "f = Counter(train_dataset)\n",
        "p = torch.zeros(len(VOCAB))\n",
        "\n",
        "# Downsample frequent words and upsample less frequent.\n",
        "s = sum([np.power(freq, 0.75) for word, freq in f.items()])\n",
        "\n",
        "for word in f:\n",
        "    p[stoi[word]] = np.power(f[word], 0.75) / s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4a2e7296",
      "metadata": {
        "id": "4a2e7296"
      },
      "outputs": [],
      "source": [
        "# Map everything to integers.\n",
        "train_dataset = [stoi[word] for word in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8e96c0c4",
      "metadata": {
        "id": "8e96c0c4"
      },
      "outputs": [],
      "source": [
        "# This just gets the (wc, wo) pairs that are positive - they are seen together!\n",
        "def get_tokenized_dataset(dataset, verbose=False):\n",
        "    x_list = []\n",
        "\n",
        "    for i, token in enumerate(dataset):\n",
        "        # Pick a RANDOM window around the center word.\n",
        "        m = random.randint(1, 5)\n",
        "\n",
        "        start = max(0,i-m)\n",
        "\n",
        "        end = min(i+m,len(dataset)-1)\n",
        "\n",
        "        target_tokens = dataset[start:i] + dataset[i+1:end+1]\n",
        "\n",
        "        wc = token\n",
        "\n",
        "        x_list.extend([\n",
        "            [wc, wo] for wo in target_tokens\n",
        "        ])\n",
        "\n",
        "    return x_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb82aad",
      "metadata": {
        "id": "3eb82aad"
      },
      "outputs": [],
      "source": [
        "train_x_list = get_tokenized_dataset(train_dataset, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8412ee4d",
      "metadata": {
        "id": "8412ee4d"
      },
      "outputs": [],
      "source": [
        "pickle.dump(train_x_list, open('train_x_list.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd54caf",
      "metadata": {
        "id": "3fd54caf"
      },
      "outputs": [],
      "source": [
        "train_x_list = pickle.load(open('train_x_list.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576765f3",
      "metadata": {
        "id": "576765f3"
      },
      "outputs": [],
      "source": [
        "# These are (wc, wo) pairs. All are y = +1 by design.\n",
        "train_x_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8137dbd6",
      "metadata": {
        "id": "8137dbd6"
      },
      "outputs": [],
      "source": [
        "# The number of things of BATCH_SIZE = 512.\n",
        "len(train_x_list) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9952d8d5",
      "metadata": {
        "id": "9952d8d5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "77f04be5",
      "metadata": {
        "id": "77f04be5"
      },
      "source": [
        "### Set up the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e31ae4b",
      "metadata": {
        "id": "2e31ae4b"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(\n",
        "    TensorDataset(\n",
        "        torch.tensor(train_x_list).to(DEVICE),\n",
        "    ),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c949153",
      "metadata": {
        "id": "8c949153"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "12d93d22",
      "metadata": {
        "id": "12d93d22"
      },
      "source": [
        "### Words we'll use to asses the quality of the model ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffada8d0",
      "metadata": {
        "id": "ffada8d0"
      },
      "outputs": [],
      "source": [
        "valid_ids = torch.tensor([\n",
        "    stoi['money'],\n",
        "    stoi['lion'],\n",
        "    stoi['africa'],\n",
        "    stoi['musician'],\n",
        "    stoi['dance'],\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13f64b87",
      "metadata": {
        "id": "13f64b87"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64096cd8",
      "metadata": {
        "id": "64096cd8"
      },
      "source": [
        "### Get the model\n",
        "- See docs for nn.Embedding: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "- The same: nn.Embedding (2) = nn.Embedding.weight * oneHot(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc51c359",
      "metadata": {
        "id": "dc51c359"
      },
      "outputs": [],
      "source": [
        "class SkipGramNegativeSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(SkipGramNegativeSampling, self).__init__()\n",
        "        self.A = nn.Embedding(vocab_size, embed_dim) # Context vectors - center word.\n",
        "        self.B = nn.Embedding(vocab_size, embed_dim) # Output vectors - words around the center word.\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Is this the best way? Not sure.\n",
        "        initrange = 0.5\n",
        "        self.A.weight.data.uniform_(-initrange, initrange)\n",
        "        self.B.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # N is the batch size.\n",
        "        # x is (N, 2)\n",
        "\n",
        "        # Each of these is (N, 1)\n",
        "        wc, wo = x[:, 0], x[:, 1]\n",
        "\n",
        "        # Each of these is (N, 1, D) since each context has 1 word.\n",
        "        a = self.A(wc)\n",
        "\n",
        "        # Each of these is (N, 1, D) since each target has 1 word.\n",
        "        b = self.B(wo)\n",
        "\n",
        "        # The product between each context and target vector.\n",
        "        # Each of these is (N, 1, D) since each batch has 1 word.\n",
        "        # The logits is now (N, 1) since we sum across the final dimension.\n",
        "        logits = (a * b).sum(axis=-1)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae817c1",
      "metadata": {
        "id": "7ae817c1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9c509d",
      "metadata": {
        "id": "ed9c509d"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_embeddings(\n",
        "    model,\n",
        "    valid_ids,\n",
        "    itos\n",
        "):\n",
        "    \"\"\" Validation logic \"\"\"\n",
        "\n",
        "    # We will use context embeddings to get the most similar words\n",
        "    # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
        "    embedding_weights = model.A.weight\n",
        "\n",
        "    normalized_embeddings = embedding_weights.cpu() / np.sqrt(\n",
        "        np.sum(embedding_weights.cpu().numpy()**2, axis=1, keepdims=True)\n",
        "    )\n",
        "\n",
        "    # Get the embeddings corresponding to valid_term_ids\n",
        "    valid_embeddings = normalized_embeddings[valid_ids, :]\n",
        "\n",
        "    # Compute the similarity between valid_term_ids (S) and all the embeddings (V).\n",
        "    # We do S x d (d x V) => S x D and sort by negative similarity.\n",
        "    top_k = 10 # Top k items will be displayed.\n",
        "    similarity = np.dot(valid_embeddings.cpu().numpy(), normalized_embeddings.cpu().numpy().T)\n",
        "\n",
        "    # Invert similarity matrix to negative\n",
        "    # Ignore the first one because that would be the same word as the probe word\n",
        "    similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
        "\n",
        "    # Print the output.\n",
        "    for i, word_id in enumerate(valid_ids):\n",
        "        # j >= 1 here since we don't want to include the word itself.\n",
        "        similar_word_str = ', '.join([itos[j] for j in similarity_top_k[i, :] if j >= 1])\n",
        "        print(f\"{itos[word_id]}: {similar_word_str}\")\n",
        "\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c194b8",
      "metadata": {
        "id": "b0c194b8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3b3c6ed5",
      "metadata": {
        "id": "3b3c6ed5"
      },
      "source": [
        "### Set up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95118499",
      "metadata": {
        "id": "95118499"
      },
      "outputs": [],
      "source": [
        "LR = 10.0\n",
        "NUM_EPOCHS = 10\n",
        "EMBED_DIM = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef585f4",
      "metadata": {
        "id": "cef585f4"
      },
      "outputs": [],
      "source": [
        "model = SkipGramNegativeSampling(len(VOCAB), EMBED_DIM).to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "# The learning rate is lowered every epoch by 1/10.\n",
        "# Is this a good idea?\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a642bf",
      "metadata": {
        "id": "f8a642bf"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85773616",
      "metadata": {
        "id": "85773616"
      },
      "outputs": [],
      "source": [
        "validate_embeddings(model, valid_ids, itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7867ee3c",
      "metadata": {
        "id": "7867ee3c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "86476e2a",
      "metadata": {
        "id": "86476e2a"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24950481",
      "metadata": {
        "id": "24950481"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, optimizer, epoch):\n",
        "    model.train()\n",
        "    total_acc, total_count, total_loss, total_batches = 0, 0, 0.0, 0.0\n",
        "    log_interval = 500\n",
        "\n",
        "    for idx, x_batch in tqdm(enumerate(dataloader)):\n",
        "\n",
        "        x_batch = x_batch[0]\n",
        "\n",
        "        batch_size = x_batch.shape[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(x_batch)\n",
        "\n",
        "        # Get the positive samples loss. Notice we use weights here\n",
        "        positive_loss = torch.nn.BCEWithLogitsLoss()(input=logits, target=torch.ones(batch_size).to(DEVICE).float())\n",
        "\n",
        "        # For each batch, get some negative samples\n",
        "        # We need a total of B * 2 * WINDOW * K samples across a batch\n",
        "        # We then reshape this batch\n",
        "        # These are effectively the output words\n",
        "        negative_samples = torch.multinomial(p, batch_size * K, replacement=True)\n",
        "\n",
        "        # Repeat the center word enough times so we can merge as needed\n",
        "        # Repeat does not work on MPS apparently\n",
        "        wc = x_batch[:, 0].to(\"cpu\").repeat(K).sort().values\n",
        "        wo = negative_samples\n",
        "\n",
        "        # Get the negative samples\n",
        "        x_batch_negative = torch.stack([wc, wo], axis=1).to(DEVICE)\n",
        "\n",
        "        \"\"\"\n",
        "        Note the way we formulated the targets: they are all 0 since these are negative samples.\n",
        "        We do the BCEWithLogitsLoss by hand basically here.\n",
        "        Notice we sum across the negative samples, per positive word.\n",
        "\n",
        "        This is literally the equation in the lecture notes.\n",
        "        \"\"\"\n",
        "        negative_loss = model(x_batch_negative).neg().sigmoid().log().reshape(\n",
        "            batch_size, K\n",
        "        ).sum(1).mean().neg().to(DEVICE)\n",
        "\n",
        "        loss = (positive_loss + negative_loss).mean()\n",
        "\n",
        "        # Get the gradients via back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients? Generally a good idea\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "        # Do an optimization step. Update the parameters A and B\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "        if idx % log_interval == 0:\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| loss {:8.3f} \".format(\n",
        "                    epoch,\n",
        "                    idx,\n",
        "                    len(dataloader),\n",
        "                    total_loss / total_batches\n",
        "                )\n",
        "            )\n",
        "            validate_embeddings(model, valid_ids, itos)\n",
        "            total_loss, total_batches = 0.0, 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fe7e0d",
      "metadata": {
        "id": "61fe7e0d"
      },
      "source": [
        "### Some results from the run look like below:\n",
        "\n",
        "![Results.png](attachment:Results.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e02c09",
      "metadata": {
        "id": "a9e02c09"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train(train_dl, model, optimizer, epoch)\n",
        "    # We have a learning rate scheduler here\n",
        "    # Basically, given the state of the optimizer, this lowers the learning rate in a smart way\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0227888",
      "metadata": {
        "id": "b0227888"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}